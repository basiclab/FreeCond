{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/bluedyee/miniconda3/envs/fluxinp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùó‚ùó‚ùó Be sure using correct python environment, the python environment are different for methods \n",
      "üîÑ Building Stable-Diffusion-Inpainting FreeCond control...\n",
      "# Load backbone model\n",
      "## Load backbone model = diffusers/stable-diffusion-xl-1.0-inpainting-0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|‚ñà‚ñà‚ñä       | 2/7 [00:00<00:00, 13.32it/s]The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "Loading pipeline components...:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4/7 [00:00<00:00, 15.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# pipeline, forward = get_pipeline_forward(method=\"sd\",variant=\"sd15\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# pipeline, forward = get_pipeline_forward(method=\"cn\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# pipeline, forward = get_pipeline_forward(method=\"bn\",device=\"cuda\")\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m pipeline, forward \u001b[38;5;241m=\u001b[39m \u001b[43mget_pipeline_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msdxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# pipeline.scheduler=DDIMScheduler.from_config(pipeline.scheduler.config) # This was used during the experiment\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# pipeline, forward = get_pipeline_forward(method=\"flux\", device=\"cuda\")\u001b[39;00m\n",
      "File \u001b[0;32m~/github_repo/INPAINTING/FreeCond/freecond_src/freecond_utils.py:346\u001b[0m, in \u001b[0;36mget_pipeline_forward\u001b[0;34m(method, variant, device, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÑ Building Stable-Diffusion-Inpainting FreeCond control...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 346\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m \u001b[43mget_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    349\u001b[0m         fc_control,\n\u001b[1;32m    350\u001b[0m         init_image,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargss\n\u001b[1;32m    358\u001b[0m     ):\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pipe\u001b[38;5;241m.\u001b[39mfreecond_forward_staged(\n\u001b[1;32m    360\u001b[0m             fc_control,\n\u001b[1;32m    361\u001b[0m             prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargss\n\u001b[1;32m    368\u001b[0m         )\n",
      "File \u001b[0;32m~/github_repo/INPAINTING/FreeCond/freecond_src/freecond.py:2740\u001b[0m, in \u001b[0;36mget_pipeline\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdxl\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2737\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   2738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Load backbone model = diffusers/stable-diffusion-xl-1.0-inpainting-0.1 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2739\u001b[0m     )\n\u001b[0;32m-> 2740\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m \u001b[43mFreeCondXLPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiffusers/stable-diffusion-xl-1.0-inpainting-0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2746\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Load backbone model = Stable Diffusion Inpainting 1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:876\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    873\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m passed_class_obj[name]\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    899\u001b[0m     )\n\u001b[1;32m    901\u001b[0m init_kwargs[name] \u001b[38;5;241m=\u001b[39m loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/pipelines/pipeline_loading_utils.py:700\u001b[0m, in \u001b[0;36mload_sub_model\u001b[0;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m# check if the module is in a subdirectory\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name)):\n\u001b[0;32m--> 700\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloading_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(cached_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/modeling_utils.py:730\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;66;03m# Instantiate model with empty weights\u001b[39;00m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerate\u001b[38;5;241m.\u001b[39minit_empty_weights():\n\u001b[0;32m--> 730\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# if device_map is None, load the state dict and move the params from meta device to the cpu\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded:\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/configuration_utils.py:260\u001b[0m, in \u001b[0;36mConfigMixin.from_config\u001b[0;34m(cls, config, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m         init_dict[deprecated_kwarg] \u001b[38;5;241m=\u001b[39m unused_kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_kwarg)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Return model and optionally state and/or unused_kwargs\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# make sure to also save config parameters that might be used for compatible classes\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# update _class_name\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hidden_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/configuration_utils.py:653\u001b[0m, in \u001b[0;36mregister_to_config.<locals>.inner_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_init_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs}\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_to_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n\u001b[0;32m--> 653\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:385\u001b[0m, in \u001b[0;36mUNet2DConditionModel.__init__\u001b[0;34m(self, sample_size, in_channels, out_channels, center_input_sample, flip_sin_to_cos, freq_shift, down_block_types, mid_block_type, up_block_types, only_cross_attention, block_out_channels, layers_per_block, downsample_padding, mid_block_scale_factor, dropout, act_fn, norm_num_groups, norm_eps, cross_attention_dim, transformer_layers_per_block, reverse_transformer_layers_per_block, encoder_hid_dim, encoder_hid_dim_type, attention_head_dim, num_attention_heads, dual_cross_attention, use_linear_projection, class_embed_type, addition_embed_type, addition_time_embed_dim, num_class_embeds, upcast_attention, resnet_time_scale_shift, resnet_skip_time_act, resnet_out_scale_factor, time_embedding_type, time_embedding_dim, time_embedding_act_fn, timestep_post_act, time_cond_proj_dim, conv_in_kernel, conv_out_kernel, projection_class_embeddings_input_dim, attention_type, class_embeddings_concat, mid_block_only_cross_attention, cross_attention_norm, addition_embed_type_num_heads)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks\u001b[38;5;241m.\u001b[39mappend(down_block)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# mid\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block \u001b[38;5;241m=\u001b[39m \u001b[43mget_mid_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmid_block_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemb_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks_time_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_out_channels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_act_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_num_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmid_block_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer_layers_per_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_layers_per_block\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_dim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdual_cross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdual_cross_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_linear_projection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_linear_projection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmid_block_only_cross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmid_block_only_cross_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupcast_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcast_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_time_scale_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_time_scale_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_skip_time_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_skip_time_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_head_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_head_dim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# count how many layers upsample the images\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_upsamplers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:275\u001b[0m, in \u001b[0;36mget_mid_block\u001b[0;34m(mid_block_type, temb_channels, in_channels, resnet_eps, resnet_act_fn, resnet_groups, output_scale_factor, transformer_layers_per_block, num_attention_heads, cross_attention_dim, dual_cross_attention, use_linear_projection, mid_block_only_cross_attention, upcast_attention, resnet_time_scale_shift, attention_type, resnet_skip_time_act, cross_attention_norm, attention_head_dim, dropout)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_mid_block\u001b[39m(\n\u001b[1;32m    253\u001b[0m     mid_block_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    254\u001b[0m     temb_channels: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     dropout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    273\u001b[0m ):\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mid_block_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNetMidBlock2DCrossAttn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUNetMidBlock2DCrossAttn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransformer_layers_per_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_layers_per_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemb_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresnet_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresnet_act_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_act_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresnet_time_scale_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_time_scale_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresnet_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdual_cross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdual_cross_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_linear_projection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_linear_projection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupcast_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcast_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mid_block_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNetMidBlock2DSimpleCrossAttn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m UNetMidBlock2DSimpleCrossAttn(\n\u001b[1;32m    294\u001b[0m             in_channels\u001b[38;5;241m=\u001b[39min_channels,\n\u001b[1;32m    295\u001b[0m             temb_channels\u001b[38;5;241m=\u001b[39mtemb_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m             cross_attention_norm\u001b[38;5;241m=\u001b[39mcross_attention_norm,\n\u001b[1;32m    307\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:804\u001b[0m, in \u001b[0;36mUNetMidBlock2DCrossAttn.__init__\u001b[0;34m(self, in_channels, temb_channels, out_channels, dropout, num_layers, transformer_layers_per_block, resnet_eps, resnet_time_scale_shift, resnet_act_fn, resnet_groups, resnet_groups_out, resnet_pre_norm, num_attention_heads, output_scale_factor, cross_attention_dim, dual_cross_attention, use_linear_projection, upcast_attention, attention_type)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers):\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dual_cross_attention:\n\u001b[1;32m    803\u001b[0m         attentions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 804\u001b[0m             \u001b[43mTransformer2DModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m                \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m                \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_layers_per_block\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcross_attention_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnorm_num_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_groups_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_linear_projection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_linear_projection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m                \u001b[49m\u001b[43mupcast_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcast_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattention_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m         )\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    817\u001b[0m         attentions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    818\u001b[0m             DualTransformer2DModel(\n\u001b[1;32m    819\u001b[0m                 num_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m             )\n\u001b[1;32m    826\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/configuration_utils.py:653\u001b[0m, in \u001b[0;36mregister_to_config.<locals>.inner_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_init_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs}\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_to_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n\u001b[0;32m--> 653\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:168\u001b[0m, in \u001b[0;36mTransformer2DModel.__init__\u001b[0;34m(self, num_attention_heads, attention_head_dim, in_channels, out_channels, num_layers, dropout, norm_num_groups, cross_attention_dim, attention_bias, sample_size, num_vector_embeds, patch_size, activation_fn, num_embeds_ada_norm, use_linear_projection, only_cross_attention, double_self_attention, upcast_attention, norm_type, norm_elementwise_affine, norm_eps, attention_type, caption_channels, interpolation_scale, use_additional_conditions)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# 2. Initialize the right blocks.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# These functions follow a common structure:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# a. Initialize the input blocks. b. Initialize the transformer blocks.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# c. Initialize the output blocks and other projection blocks when necessary.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_continuous_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_vectorized:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_vectorized_inputs(norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:184\u001b[0m, in \u001b[0;36mTransformer2DModel._init_continuous_input\u001b[0;34m(self, norm_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 184\u001b[0m     [\n\u001b[1;32m    185\u001b[0m         BasicTransformerBlock(\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim,\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_head_dim,\n\u001b[1;32m    189\u001b[0m             dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m    190\u001b[0m             cross_attention_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcross_attention_dim,\n\u001b[1;32m    191\u001b[0m             activation_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mactivation_fn,\n\u001b[1;32m    192\u001b[0m             num_embeds_ada_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_embeds_ada_norm,\n\u001b[1;32m    193\u001b[0m             attention_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias,\n\u001b[1;32m    194\u001b[0m             only_cross_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39monly_cross_attention,\n\u001b[1;32m    195\u001b[0m             double_self_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdouble_self_attention,\n\u001b[1;32m    196\u001b[0m             upcast_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupcast_attention,\n\u001b[1;32m    197\u001b[0m             norm_type\u001b[38;5;241m=\u001b[39mnorm_type,\n\u001b[1;32m    198\u001b[0m             norm_elementwise_affine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnorm_elementwise_affine,\n\u001b[1;32m    199\u001b[0m             norm_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnorm_eps,\n\u001b[1;32m    200\u001b[0m             attention_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_type,\n\u001b[1;32m    201\u001b[0m         )\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_layers)\n\u001b[1;32m    203\u001b[0m     ]\n\u001b[1;32m    204\u001b[0m )\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_linear_projection:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:185\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    184\u001b[0m     [\n\u001b[0;32m--> 185\u001b[0m         \u001b[43mBasicTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_head_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_embeds_ada_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_embeds_ada_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43monly_cross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monly_cross_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdouble_self_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_self_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupcast_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupcast_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnorm_elementwise_affine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_elementwise_affine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnorm_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_layers)\n\u001b[1;32m    203\u001b[0m     ]\n\u001b[1;32m    204\u001b[0m )\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_linear_projection:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/attention.py:362\u001b[0m, in \u001b[0;36mBasicTransformerBlock.__init__\u001b[0;34m(self, dim, num_attention_heads, attention_head_dim, dropout, cross_attention_dim, activation_fn, num_embeds_ada_norm, attention_bias, only_cross_attention, double_self_attention, upcast_attention, norm_elementwise_affine, norm_type, norm_eps, final_dropout, attention_type, positional_embeddings, num_positional_embeddings, ada_norm_continous_conditioning_embedding_dim, ada_norm_bias, ff_inner_dim, ff_bias, attention_out_bias)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(dim, norm_eps, norm_elementwise_affine)\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2 \u001b[38;5;241m=\u001b[39m \u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdouble_self_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_head_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupcast_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcast_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_out_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# is self-attn if encoder_hidden_states is none\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_single\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# For Latte\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/diffusers/models/attention_processor.py:238\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[0;34m(self, query_dim, cross_attention_dim, heads, kv_heads, dim_head, dropout, bias, upcast_attention, upcast_softmax, cross_attention_norm, cross_attention_norm_num_groups, qk_norm, added_kv_proj_dim, added_proj_bias, norm_num_groups, spatial_norm_dim, out_bias, scale_qk, only_cross_attention, eps, rescale_output_factor, residual_connection, _from_deprecated_attn_block, processor, out_dim, context_pre_only, pre_only)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_q_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(added_kv_proj_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, bias\u001b[38;5;241m=\u001b[39madded_proj_bias)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_only:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_out \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModuleList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_out\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim, bias\u001b[38;5;241m=\u001b[39mout_bias))\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_out\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mDropout(dropout))\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/torch/nn/modules/container.py:308\u001b[0m, in \u001b[0;36mModuleList.__init__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, modules: Optional[Iterable[Module]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m modules\n",
      "File \u001b[0;32m~/miniconda3/envs/fluxinp/lib/python3.10/site-packages/torch/nn/modules/module.py:517\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_load_state_dict_post_hooks\u001b[39m\u001b[38;5;124m\"\u001b[39m, OrderedDict())\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m--> 517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from freecond_src.freecond_utils import get_pipeline_forward\n",
    "from freecond_src.freecond import FC_config\n",
    "from diffusers import DDIMScheduler\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "# pipeline, forward = get_pipeline_forward(method=\"sd\",variant=\"sd15\")\n",
    "\n",
    "# pipeline, forward = get_pipeline_forward(method=\"cn\")\n",
    "\n",
    "# pipeline, forward = get_pipeline_forward(method=\"hdp\",variant=\"sd15\",device=\"cuda\")\n",
    "\n",
    "# pipeline, forward = get_pipeline_forward(method=\"pp\",device=\"cuda\")\n",
    "\n",
    "# pipeline, forward = get_pipeline_forward(method=\"bn\",device=\"cuda\")\n",
    "\n",
    "pipeline, forward = get_pipeline_forward(method=\"sd\", variant=\"sdxl\")\n",
    "\n",
    "# pipeline.scheduler=DDIMScheduler.from_config(pipeline.scheduler.config) # This was used during the experiment\n",
    "\n",
    "\n",
    "# pipeline, forward = get_pipeline_forward(method=\"flux\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:03<00:00, 14.94it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:03<00:00, 15.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:03<00:00, 15.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:03<00:00, 15.37it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:03<00:00, 15.31it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'default_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 153\u001b[0m\n\u001b[1;32m    144\u001b[0m fc_out \u001b[38;5;241m=\u001b[39m forward(\n\u001b[1;32m    145\u001b[0m     fc_control,\n\u001b[1;32m    146\u001b[0m     init_image\u001b[38;5;241m=\u001b[39minit_image,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     negative_prompt\u001b[38;5;241m=\u001b[39mnprompt,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m fc_images\u001b[38;5;241m.\u001b[39mappend(fc_out[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 153\u001b[0m make_image_grid([\u001b[43mdefault_out\u001b[49m[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m fc_images, rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'default_out' is not defined"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import make_image_grid\n",
    "import torch\n",
    "\n",
    "# ‚ùó The demonstration output aren't identical the reported images in our research paper.\n",
    "# Because we delete some unnecessary operation during revision, leading to the change of effect about random seed.\n",
    "\n",
    "mask = Image.open(\"./demo_data/mask_1_2.png\").convert(\"L\").resize((512, 512))\n",
    "init_image = Image.open(\"./demo_data/img_1_2.jpg\").convert(\"RGB\").resize((512, 512))\n",
    "prompt = \"A golden retriever wearing astronaut gear, in cyberpunk style\"\n",
    "nprompt = \"word, bad quality, bad anatomy, ugly, mutation, blurry, error\"\n",
    "\n",
    "# Default to SDI\n",
    "fc_control = FC_config(\n",
    "    change_step=0,\n",
    "    fg_1=1,\n",
    "    fg_2=1,\n",
    "    bg_1=0,\n",
    "    bg_2=0,\n",
    "    hq_1=1,\n",
    "    hq_2=1,\n",
    "    lq_1=1,\n",
    "    lq_2=1,\n",
    "    fq_th=32,\n",
    ")\n",
    "# Setting FreeCond control for SDI15\n",
    "torch.manual_seed(777)\n",
    "default_out = forward(\n",
    "    fc_control,\n",
    "    init_image=init_image,\n",
    "    mask_image=mask,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=nprompt,\n",
    ")\n",
    "\n",
    "fc_images = []\n",
    "\n",
    "# FreeCond control for SDI 15, change alpha scale\n",
    "fc_control = FC_config(\n",
    "    change_step=0,\n",
    "    fg_1=1,\n",
    "    fg_2=3,\n",
    "    bg_1=0,\n",
    "    bg_2=0,\n",
    "    hq_1=1,\n",
    "    hq_2=1,\n",
    "    lq_1=1,\n",
    "    lq_2=1,\n",
    "    fq_th=32,\n",
    ")\n",
    "torch.manual_seed(777)\n",
    "fc_out = forward(\n",
    "    fc_control,\n",
    "    init_image=init_image,\n",
    "    mask_image=mask,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=nprompt,\n",
    ")\n",
    "fc_images.append(fc_out[0])\n",
    "\n",
    "# FreeCond control for SDI 15, change beta scale\n",
    "torch.manual_seed(777)\n",
    "fc_control = FC_config(\n",
    "    change_step=0,\n",
    "    fg_1=1,\n",
    "    fg_2=1,\n",
    "    bg_1=0,\n",
    "    bg_2=0.5,\n",
    "    hq_1=1,\n",
    "    hq_2=1,\n",
    "    lq_1=1,\n",
    "    lq_2=1,\n",
    "    fq_th=4,\n",
    ")\n",
    "fc_out = forward(\n",
    "    fc_control,\n",
    "    init_image=init_image,\n",
    "    mask_image=mask,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=nprompt,\n",
    ")\n",
    "fc_images.append(fc_out[0])\n",
    "\n",
    "# Setting FreeCond control for SDI, change freq\n",
    "torch.manual_seed(777)\n",
    "fc_control = FC_config(\n",
    "    change_step=25,\n",
    "    fg_1=1,\n",
    "    fg_2=1,\n",
    "    bg_1=0,\n",
    "    bg_2=0,\n",
    "    hq_1=0,\n",
    "    hq_2=1,\n",
    "    lq_1=1,\n",
    "    lq_2=1,\n",
    "    fq_th=24,\n",
    ")\n",
    "fc_out = forward(\n",
    "    fc_control,\n",
    "    init_image=init_image,\n",
    "    mask_image=mask,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=nprompt,\n",
    ")\n",
    "fc_images.append(fc_out[0])\n",
    "\n",
    "\n",
    "# Setting FreeCond control for SDI, hybird\n",
    "torch.manual_seed(777)\n",
    "fc_control = FC_config(\n",
    "    change_step=25,\n",
    "    fg_1=3,\n",
    "    fg_2=3,\n",
    "    bg_1=0.3,\n",
    "    bg_2=0.3,\n",
    "    hq_1=0,\n",
    "    hq_2=1,\n",
    "    lq_1=1,\n",
    "    lq_2=1,\n",
    "    fq_th=24,\n",
    ")\n",
    "fc_out = forward(\n",
    "    fc_control,\n",
    "    init_image=init_image,\n",
    "    mask_image=mask,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=nprompt,\n",
    ")\n",
    "fc_images.append(fc_out[0])\n",
    "\n",
    "# Setting FreeCond control for SDI, hybrid with stage\n",
    "torch.manual_seed(777)\n",
    "fc_control = FC_config(\n",
    "    change_step=25,\n",
    "    fg_1=1,\n",
    "    fg_2=1.5,\n",
    "    bg_1=0.2,\n",
    "    bg_2=0.2,\n",
    "    hq_1=0,\n",
    "    hq_2=1,\n",
    "    lq_1=1,\n",
    "    lq_2=1,\n",
    "    fq_th=24,\n",
    ")\n",
    "fc_out = forward(\n",
    "    fc_control,\n",
    "    init_image=init_image,\n",
    "    mask_image=mask,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=nprompt,\n",
    ")\n",
    "fc_images.append(fc_out[0])\n",
    "\n",
    "make_image_grid([default_out[0]] + fc_images, rows=2, cols=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdpainter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
